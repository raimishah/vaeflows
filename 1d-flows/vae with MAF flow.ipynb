{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import genpareto\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from anomaly_scoring import get_anomaly_scores\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "from utils import get_taxi_data_VAE, get_taxi_data_cVAE\n",
    "from utils import softclip\n",
    "from utils import plot_train_test_reconstructions, plot_train_test_reconstructions_prob_decoder_model\n",
    "from models.cnn_sigmaVAE import CNN_sigmaVAE\n",
    "\n",
    "from maf import MAF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([258, 1, 20]) torch.Size([258, 1, 20])\n",
      "torch.Size([258, 1, 20]) torch.Size([258, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../../datasets/nab/NAB-master/data/realKnownCause/nyc_taxi.csv'\n",
    "#dataset_path = '../../datasets/nab/NAB-master/data/realKnownCause/machine_temperature_system_failure.csv'\n",
    "#dataset_path = '../../datasets/nab/NAB-master/data/realKnownCause/ambient_temperature_system_failure.csv'\n",
    "#dataset_path = '../../datasets/nab/NAB-master/data/realKnownCause/cpu_utilization_asg_misconfiguration.csv'\n",
    "#dataset_path = '../../datasets/nab/NAB-master/data/realKnownCause/ec2_request_latency_system_failure.csv'\n",
    "\n",
    "\n",
    "if 'nyc_taxi' in dataset_path:\n",
    "    anomaly_timestamps = [\"2014-11-01 19:00:00\", \"2014-11-27 15:30:00\",\"2014-12-25 15:00:00\",\"2015-01-01 01:00:00\",\"2015-01-27 00:00:00\"]\n",
    "\n",
    "    anomaly_windows = [\n",
    "        [\n",
    "            \"2014-10-30 15:30:00\",\n",
    "            \"2014-11-03 22:30:00\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-11-25 12:00:00\",\n",
    "            \"2014-11-29 19:00:00\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-12-23 11:30:00\",\n",
    "            \"2014-12-27 18:30:00\"\n",
    "        ],\n",
    "        [\n",
    "            \"2014-12-29 21:30:00\",\n",
    "            \"2015-01-03 04:30:00\"\n",
    "        ],\n",
    "        [\n",
    "            \"2015-01-24 20:30:00\",\n",
    "            \"2015-01-29 03:30:00\"\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "elif 'ambient_temperature_system_failure' in dataset_path:\n",
    "    anomaly_timestamps = [\"2013-12-22 20:00:00\",\"2014-04-13 09:00:00\"]\n",
    "\n",
    "elif 'cpu_utilization_asg_misconfiguration' in dataset_path:\n",
    "    anomaly_timestamps = [\"2014-07-12 02:04:00\",\"2014-07-14 21:44:00\"]\n",
    "\n",
    "elif 'ec2_request_latency_system_failure' in dataset_path:\n",
    "    anomaly_timestamps = [\"2014-03-14 09:06:00\",\"2014-03-18 22:41:00\",\"2014-03-21 03:01:00\"]\n",
    "\n",
    "elif 'machine_temperature_system_failure' in dataset_path:\n",
    "    anomaly_timestamps = [\"2013-12-11 06:00:00\",\"2013-12-16 17:25:00\",\"2014-01-28 13:55:00\",\"2014-02-08 14:30:00\"]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#singular anomaly points\n",
    "'''\n",
    "window_size=128\n",
    "X_train_data, X_test_data, X_train_tensor, X_test_tensor, trainloader, testloader = get_taxi_data_VAE(dataset_path,window_size=window_size,train_test_split=.5) \n",
    "\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "timestamps = list(pd.read_csv(dataset_path)['timestamp'])\n",
    "    \n",
    "anomaly_idxs = []\n",
    "for timestamp in anomaly_timestamps:\n",
    "    start = timestamp\n",
    "\n",
    "    loc_start = timestamps.index(start)\n",
    "    anomaly_idxs.append(loc_start)\n",
    "    \n",
    "anomaly_idxs = np.array(anomaly_idxs)    \n",
    "'''\n",
    "\n",
    "\n",
    "#'''\n",
    "\n",
    "window_size=20\n",
    "X_train_data, X_test_data, X_train_tensor, X_test_tensor, trainloader, testloader = get_taxi_data_VAE(dataset_path,window_size=window_size,train_test_split=.5) \n",
    "\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "timestamps = list(pd.read_csv(dataset_path)['timestamp'])\n",
    "\n",
    "anomaly_idxs = []\n",
    "for timestamp in anomaly_windows:\n",
    "    start, end = timestamp[0], timestamp[1]\n",
    "\n",
    "    loc_start = timestamps.index(start)\n",
    "    loc_end = timestamps.index(end)\n",
    "\n",
    "    for i in range(loc_start, loc_end):\n",
    "        anomaly_idxs.append(i)\n",
    "\n",
    "anomaly_idxs = np.array(anomaly_idxs)\n",
    "anomaly_idxs = anomaly_idxs[anomaly_idxs > 0]\n",
    "\n",
    "plt.figure(figsize=(30,9))\n",
    "for ai in anomaly_idxs:\n",
    "    plt.scatter(ai, 1.7)\n",
    "\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_sigmaVAE_flow(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_dim=8, window_size=20, use_probabilistic_decoder=False, flow_type = 'MAF'):\n",
    "        super(CNN_sigmaVAE_flow, self).__init__()\n",
    "        \n",
    "        self.window_size=window_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prob_decoder = use_probabilistic_decoder\n",
    "        self.flow_type=flow_type\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        self.fc41 = nn.Linear(4*8, self.latent_dim)\n",
    "        self.fc42 = nn.Linear(4*8, self.latent_dim)\n",
    "\n",
    "        self.defc1 = nn.Linear(self.latent_dim, 4*8)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose1d(in_channels=4, out_channels=16, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
    "        self.deconv3 = nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=5, stride=1, padding=0, output_padding=0)\n",
    "\n",
    "        self.decoder_fc41 = nn.Linear(window_size, window_size)\n",
    "        self.decoder_fc42 = nn.Linear(window_size, window_size)\n",
    "        \n",
    "        self.decoder_fc43 = nn.Linear(window_size, window_size)\n",
    "        self.decoder_fc44 = nn.Linear(window_size, window_size)\n",
    "        \n",
    "        self.log_sigma = torch.nn.Parameter(torch.full((1,), 0.0)[0], requires_grad=True)\n",
    "        \n",
    "        if self.flow_type=='RealNVP':\n",
    "            self.flow = RealNVP(n_blocks=1, input_size=self.latent_dim, hidden_size=50, n_hidden=1)\n",
    "        \n",
    "        elif self.flow_type=='MAF':\n",
    "            self.flow = MAF(n_blocks=1, input_size=self.latent_dim, hidden_size=50, n_hidden=1)\n",
    "        \n",
    "        \n",
    "    def encoder(self, x):\n",
    "        concat_input = x #torch.cat([x, c], 1)\n",
    "        h = F.relu(self.conv1(concat_input))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        \n",
    "        self.saved_dim = [h.size(1), h.size(2)]\n",
    "        \n",
    "        h = h.view(h.size(0), h.size(1) * h.size(2))\n",
    "        \n",
    "        return self.fc41(h), self.fc42(h)\n",
    "    \n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        concat_input = z #torch.cat([z, c], 1)\n",
    "        concat_input = self.defc1(concat_input)\n",
    "        concat_input = concat_input.view(concat_input.size(0), self.saved_dim[0], self.saved_dim[1])\n",
    "        \n",
    "        h = F.relu(self.deconv1(concat_input))\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        \n",
    "        out = torch.sigmoid(self.deconv3(h))\n",
    "        \n",
    "        if self.prob_decoder:\n",
    "            rec_mu = self.decoder_fc43(out).tanh()\n",
    "            rec_sigma = self.decoder_fc44(out).tanh()\n",
    "            return out, rec_mu, rec_sigma\n",
    "        \n",
    "        #else:\n",
    "        return out, 0, 0\n",
    "    \n",
    "    \n",
    "    def latent_not_planar(self, x, z_params):\n",
    "        n_batch = x.size(0)\n",
    "                \n",
    "        # Retrieve set of parameters\n",
    "        #mu, sigma = z_params\n",
    "        mu, log_var = z_params\n",
    "        sigma = torch.sqrt(log_var.exp())\n",
    "        \n",
    "        # Obtain our first set of latent points\n",
    "        z0 = self.sampling(mu, log_var)\n",
    "        \n",
    "        zk, loss = self.flow.log_prob(z0, None)\n",
    "        loss = -loss.mean(0)\n",
    "\n",
    "        return zk, loss\n",
    "            \n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #mu, log_var = self.encoder(x)\n",
    "        #z = self.sampling(mu, log_var)\n",
    "        #output = self.decoder(z)\n",
    "        #return output, mu, log_var\n",
    "        \n",
    "        z_params = self.encoder(x)\n",
    "        mu, log_var = z_params\n",
    "        \n",
    "        z_k, kl = self.latent_not_planar(x, z_params)\n",
    "        \n",
    "        output, rec_mu, rec_sigma = self.decoder(z_k)\n",
    "   \n",
    "        return output, rec_mu, rec_sigma, kl\n",
    "        \n",
    "        \n",
    "        \n",
    "    def gaussian_nll(self, mu, log_sigma, x):\n",
    "        return 0.5 * torch.pow((x - mu) / log_sigma.exp(), 2) + log_sigma + 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "    \n",
    "    def reconstruction_loss(self, x_hat, x):\n",
    "\n",
    "        log_sigma = self.log_sigma\n",
    "        log_sigma = softclip(log_sigma, -6)\n",
    "        \n",
    "        rec_comps = self.gaussian_nll(x_hat, log_sigma, x)\n",
    "        rec = rec_comps.sum()\n",
    "\n",
    "        return rec_comps, rec\n",
    "\n",
    "    \n",
    "    def loss_function(self, recon_x, x, rec_mu, rec_sigma, kl):\n",
    "        \n",
    "        rec_comps, rec = self.reconstruction_loss(recon_x, x)\n",
    "        #kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        rec_mu_sigma_loss = 0\n",
    "        if self.prob_decoder:\n",
    "            rec_mu_sigma_loss = self.gaussian_nll(rec_mu, rec_sigma, x).sum()\n",
    "        \n",
    "        return rec_comps, rec, rec_mu_sigma_loss, kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow_model(model, num_epochs, learning_rate, dataloader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs=num_epochs\n",
    "    tq = tqdm(range(epochs))\n",
    "    for epoch in tq:\n",
    "        flag = False\n",
    "        for j, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #batches\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda() if torch.cuda.is_available() else inputs.cpu()\n",
    "            inputs.to(device)\n",
    "            labels = labels.cuda() if torch.cuda.is_available() else labels.cpu()\n",
    "            labels.to(device)\n",
    "\n",
    "            outputs, rec_mu, rec_sigma, kl = model(inputs)\n",
    "\n",
    "            rec_comps, rec, rec_mu_sigma_loss, kl = model.loss_function(outputs, labels, rec_mu, rec_sigma, kl)\n",
    "\n",
    "            loss = rec + kl + rec_mu_sigma_loss\n",
    "\n",
    "            if(np.isnan(loss.item())):\n",
    "                print(\"Noped out at\", epoch, j, kl, rec_comps)\n",
    "                flag = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if(flag):\n",
    "            break\n",
    "        tq.set_postfix(loss=loss.item())\n",
    "        #print(epoch, 'total :' + str(loss.item()) + ' rec : ' + str(rec.item()) + ' kl : ' + str(kl.sum().item()) + ' sigma: ' + str(model.log_sigma.item()))\n",
    "\n",
    "        #break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_sigmaVAE_flow(\n",
      "  (conv1): Conv1d(1, 8, kernel_size=(5,), stride=(1,))\n",
      "  (conv2): Conv1d(8, 16, kernel_size=(5,), stride=(1,))\n",
      "  (conv3): Conv1d(16, 4, kernel_size=(5,), stride=(1,))\n",
      "  (fc41): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc42): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (defc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (deconv1): ConvTranspose1d(4, 16, kernel_size=(5,), stride=(1,))\n",
      "  (deconv2): ConvTranspose1d(16, 8, kernel_size=(5,), stride=(1,))\n",
      "  (deconv3): ConvTranspose1d(8, 1, kernel_size=(5,), stride=(1,))\n",
      "  (decoder_fc41): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (decoder_fc42): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (decoder_fc43): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (decoder_fc44): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (flow): MAF(\n",
      "    (net): FlowSequential(\n",
      "      (0): MADE(\n",
      "        (net_input): MaskedLinear(in_features=8, out_features=50, bias=True)\n",
      "        (net): Sequential(\n",
      "          (0): ReLU()\n",
      "          (1): MaskedLinear(in_features=50, out_features=50, bias=True)\n",
      "          (2): ReLU()\n",
      "          (3): MaskedLinear(in_features=50, out_features=16, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BatchNorm()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a1f0629c3040278723c5e44cee8525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN_sigmaVAE_flow()\n",
    "print(model)\n",
    "\n",
    "model = train_flow_model(model, 1500, .01, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_tensor):\n",
    "    \n",
    "    X_tensor = X_tensor.cuda() if torch.cuda.is_available() else X_tensor.cpu()\n",
    "    X_tensor.to(device)\n",
    "    out_pred, _,_,_= model(X_tensor)\n",
    "    out_pred = out_pred.cpu().detach().numpy()\n",
    "\n",
    "    idx = 0\n",
    "    preds = []\n",
    "    for i in range(len(out_pred)):\n",
    "        for j in out_pred[i,0]:\n",
    "            preds.append(j)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "'''\n",
    "returns adjusted predictions (like DONUT method) (all others paper use this I believe also)\n",
    "'''\n",
    "def evaluate_anomalies(real, scores, thresh):\n",
    "    \n",
    "    pointwise_alerts = np.array([1 if scores[i] > thresh else 0 for i in range(len(scores))])\n",
    "\n",
    "    anomaly_windows = []\n",
    "    i = 0\n",
    "    while i < len(real):\n",
    "        if real[i] == 1:\n",
    "            j = i\n",
    "            while(j < len(real)):\n",
    "                if real[j] == 0:\n",
    "                    anomaly_windows.append([i,j])\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "                if j == len(real)-1 and real[j] == 1:\n",
    "                    anomaly_windows.append([i,j+1])\n",
    "                    break                \n",
    "\n",
    "            i = j-1\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    adjusted_alerts = np.copy(pointwise_alerts)\n",
    "    for aw in anomaly_windows:\n",
    "        if pointwise_alerts[aw[0]:aw[1]].any() == 1:\n",
    "            adjusted_alerts[aw[0]:aw[1]] = 1\n",
    "\n",
    "\n",
    "    return adjusted_alerts\n",
    "\n",
    "\n",
    "def plot_error_and_anomaly_idxs(real, preds, anomaly_idxs):\n",
    "\n",
    "    plt.figure(figsize=(50,15))\n",
    "    plt.plot(real)\n",
    "    plt.plot(preds)\n",
    "    for ai in anomaly_idxs:\n",
    "        plt.plot(ai, 1)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(50,15))\n",
    "    scores = (preds - real[:len(preds)])**2\n",
    "    for idx,ai in enumerate(anomaly_idxs):\n",
    "        plt.scatter(ai, scores[ai], color='red')\n",
    "    plt.plot(scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = np.zeros(len(X_test_data), dtype=np.int)\n",
    "anomaly_idxs_test = anomaly_idxs - len(X_train_data)\n",
    "real[anomaly_idxs_test] = 1\n",
    "\n",
    "preds = evaluate_model(model, X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n",
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.9786223277909739 recall : 0.8\n",
      "aupr : 0.26955035065195565\n"
     ]
    }
   ],
   "source": [
    "plot_error_and_anomaly_idxs(X_test_data, preds, anomaly_idxs_test)\n",
    "\n",
    "real = real[:len(preds)]\n",
    "\n",
    "scores = (preds - X_test_data[:len(preds)])**2\n",
    "\n",
    "thresh = np.quantile(scores, .99)\n",
    "\n",
    "anomaly_preds = evaluate_anomalies(real, scores, thresh)\n",
    "\n",
    "plt.scatter(np.arange(len(real)),real,alpha=.5)\n",
    "plt.scatter(np.arange(len(real)),anomaly_preds,alpha=.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#rates:\n",
    "precision = precision_score(real, anomaly_preds)\n",
    "recall = recall_score(real, anomaly_preds)\n",
    "f1 = f1_score(real, anomaly_preds)\n",
    "print('precision : ' + str(precision) + ' recall : ' + str(recall))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(real, scores)\n",
    "\n",
    "#todo later\n",
    "aupr_scores = np.copy(scores)\n",
    "aupr = average_precision_score(real,scores)\n",
    "\n",
    "print('aupr : ' + str(aupr))\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
